{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuitiCNN_MU_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxLKW6Ak8QdIuWNG4UBoYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gravity0717/Capstone/blob/main/MuitiCNN_MU_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI-N16I4xX9p",
        "outputId": "bbd065c1-8ab7-4d29-b8a5-6445e862717c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "import json \n",
        "import tarfile \n",
        "import torch \n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "from PIL import Image\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "UR4z3f8bxdX9"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image \n",
        "import glob\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import numpy as np \n",
        "from torchvision import transforms\n",
        "import os \n",
        "class MU(Dataset):\n",
        "  def __init__(self,csv_path):\n",
        "    self.to_tensor=transforms.ToTensor()\n",
        "    # Read the csv file \n",
        "    self.data_info = pd.read_csv(csv_path,header=None)\n",
        "    # Read image path \n",
        "    self.img_path =  np.asarray(self.data_info.iloc[1:,6])\n",
        "    # Read the log_view < log_view as label>\n",
        "    self.label_arr =np.asarray(self.data_info.iloc[1:,7])\n",
        "    \n",
        "    \n",
        "    #data augmentation 을 기대할 수 있는 부분 ???! 고민해보자 \n",
        "    self.transformations = \\\n",
        "            transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                transforms.ToTensor()])\n",
        "    \n",
        "  \n",
        "        \n",
        "  def __len__(self):\n",
        "    return len(self.label_arr)\n",
        "  # def wait to drop \n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "   #Get image name (actually path where images are )from pandas \n",
        "    if self.img_path is not np.nan:\n",
        "      u=self.img_path[idx] + \"/u\" # 상의\n",
        "      p=self.img_path[idx] + \"/p\" # 하의\n",
        "      s=self.img_path[idx] + \"/s\" # 상의\n",
        "    else: \n",
        "      pass \n",
        "    # Open Image \n",
        "    # 이미지가 경로에 없는 겨우 대비해야 할 것 \n",
        "    if os.path.isfile(u):\n",
        "      u_as_img =Image.open(u)\n",
        "    else :\n",
        "      u_as_img =Image.new(\"RGB\", (256, 256), (255, 255, 255))\n",
        "    if os.path.isfile(p):\n",
        "      p_as_img =Image.open(p)\n",
        "    else :\n",
        "      p_as_img =Image.new(\"RGB\", (256, 256), (255, 255, 255))\n",
        "\n",
        "    if os.path.isfile(s):\n",
        "      s_as_img =Image.open(s)\n",
        "    else :\n",
        "      s_as_img =Image.new(\"RGB\", (256, 256), (255, 255, 255))\n",
        "    #Img -> to tensor \n",
        "    u_as_tensor =self.to_tensor(u_as_img)\n",
        "    p_as_tensor =self.to_tensor(p_as_img)\n",
        "    s_as_tensor =self.to_tensor(s_as_img)\n",
        "    \n",
        "\n",
        "    _label=torch.tensor(float(self.label_arr[idx]))\n",
        "    label=torch.reshape(_label,(-1,))\n",
        " \n",
        "    \n",
        "    return(u_as_tensor,p_as_tensor,s_as_tensor,label )\n",
        "\n"
      ],
      "metadata": {
        "id": "oICGND2UxtcY"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "transfer learning \n",
        "\n",
        "\n",
        "새로 훈련할 데이터가 적지만 original 데이터와 유사할 경우\n",
        "데이터의 양이 적어 fine-tune (전체 모델에 대해서 backpropagation을 진행하는 것) 은 over-fitting의 위험이 있기에 하지 않습니다.\n",
        "새로 학습할 데이터는 original 데이터와 유사하기 때문에 이 경우 최종 linear classfier 레이어만 학습을 합니다.\n",
        "\n",
        "새로 훈련할 데이터가 매우 많으며 original 데이터와 유사할 경우\n",
        "새로 학습할 데이터의 양이 많다는 것은 over-fitting의 위험이 낮다는 뜻이므로, 전체 레이어에 대해서 fine-tune을 합니다.\n",
        "\n",
        "새로 훈련할 데이터가 적으며 original 데이터와 다른 경우\n",
        "데이터의 양이 적기 때문에 최종 단계의 linear classifier 레이어를 학습하는 것이 좋을 것입니다. 반면서 데이터가 서로 다르기 때문에 거의 마지막부분 (the top of the network)만 학습하는 것은 좋지 않습니다. 서로 상충이 되는데.. 이 경우에는 네트워크 초기 부분 어딘가 activation 이후에 특정 레이어를 학습시키는게 좋습니다.\n",
        "\n",
        "새로 훈련할 데이터가 많지만 original 데이터와와 다른 경우\n",
        "데이터가 많기 때문에 아예 새로운 ConvNet을 만들수도 있지만, 실적적으로 transfer learning이 더 효율이 좋습니다. 전체 네트워크에 대해서 fine-tune을 해도 됩니다.\n",
        "\n",
        "'''\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "model_pt=models.vgg16(pretrained=True)\n",
        "\n",
        "\n",
        "for p in model_pt.parameters():\n",
        "  p.requires_grad=False # 가중치가 학습되지 않도록 함 \n",
        "#in_features?\n",
        "\n",
        "\n",
        "print(model_pt.features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwqge-pmxvnp",
        "outputId": "de63e9df-f33a-4e0a-a962-4ad0665ac6e7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "pretrained model VGG 16 VGG 19 AlexNet LeNet .... Transfer learning \n",
        "'''\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.l1   =  nn.Linear(98304 ,1024)\n",
        "        self.l2   =  nn.Linear(1024,1) # regression 이기 때문에 마지막 차원 1 \n",
        "        self.VGG16 = model_pt.features\n",
        "        \n",
        "    def forward(self, top ,bot,shoes):\n",
        "        top= self.VGG16(top)   #216*216\n",
        "        bot = self.VGG16(bot)  #\n",
        "        shoes = self.VGG16(shoes) #\n",
        "        #outer=self.VGG16(outer)   #\n",
        "        N ,_,_,_ = top.size()\n",
        "        top = top.view(N,-1)\n",
        "        bot = bot.view(N,-1)\n",
        "        shoes = shoes.view(N,-1)\n",
        "        #outer = outer.view(N,-1)\n",
        "        z = torch.cat((top,bot,shoes),1)\n",
        "        z = self.l1(z)\n",
        "        z = self.relu(z)\n",
        "        z = self.l2(z)\n",
        "        return z\n",
        "    \n"
      ],
      "metadata": {
        "id": "xOe-oAwvxwO4"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model().to(device)"
      ],
      "metadata": {
        "id": "jJ95PNibx2JT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion =torch.nn.MSELoss()\n",
        "optimizer =torch.optim.SGD(model.parameters(), lr=  0.01)"
      ],
      "metadata": {
        "id": "5ti-0JK6x2bm"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/캡스톤/label_and_path_withoutNaN\")\n",
        "df.iloc[:,6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO2kHMnE4qG9",
        "outputId": "5962bc25-60c8-457d-9a35-e1818066dc5b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          /content/drive/MyDrive/캡스톤/data/1\n",
              "1          /content/drive/MyDrive/캡스톤/data/3\n",
              "2          /content/drive/MyDrive/캡스톤/data/4\n",
              "3          /content/drive/MyDrive/캡스톤/data/6\n",
              "4          /content/drive/MyDrive/캡스톤/data/7\n",
              "                          ...                    \n",
              "2286    /content/drive/MyDrive/캡스톤/data/7078\n",
              "2287    /content/drive/MyDrive/캡스톤/data/7080\n",
              "2288    /content/drive/MyDrive/캡스톤/data/7081\n",
              "2289    /content/drive/MyDrive/캡스톤/data/7082\n",
              "2290    /content/drive/MyDrive/캡스톤/data/7092\n",
              "Name: img_path, Length: 2291, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "\n",
        "custom_cloth = MU(\"/content/drive/MyDrive/캡스톤/label_and_path_withoutNaN\")\n",
        "    # Define dataloader\n",
        "\n",
        "/content/drive/MyDrive/캡스톤/\n",
        "dataset_size = len(custom_cloth)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "train_dataset,  test_dataset = random_split(custom_cloth, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "T9X-GHHSx4-o"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def test_prediction(model,  u,p,s ):\n",
        "    # generating predictions for test set\n",
        "    with torch.no_grad():\n",
        "        output = model(u,p,s)\n",
        "\n",
        "    softmax = torch.exp(output)\n",
        "    prob = list(softmax)\n",
        "    prob=torch.Tensor(prob)\n",
        "    predictions =torch.argmax(prob) # argmax (최대값의 색인을 리턴)\n",
        "\n",
        "    return predictions\n",
        "\"\"\"\n",
        "def train(n_epochs,model, optimizer,criterion):\n",
        "    rmse_history = []\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        \n",
        "        dataiter=iter(train_dataloader)\n",
        "        u,p,s ,label = dataiter.next()\n",
        "        # clearing the Gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prediction for training and validation set\n",
        "        model.train()\n",
        "        u=u.type(torch.cuda.FloatTensor)\n",
        "        p=p.type(torch.cuda.FloatTensor)\n",
        "        s=s.type(torch.cuda.FloatTensor)\n",
        "        label=label.to(device)\n",
        "        output_train = model(u,p,s)\n",
        "\n",
        "        # computing the training and validation loss\n",
        "        loss_train = criterion(output_train, label)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10== 0:\n",
        "            model.eval()\n",
        "            test_loss = 0.\n",
        "\n",
        "            testiter=iter(test_dataloader)\n",
        "            u,p,s ,y_test=testiter.next()\n",
        "            y_test=y_test.detach().cpu().numpy()    \n",
        "            u=u.type(torch.cuda.FloatTensor)\n",
        "            p=p.type(torch.cuda.FloatTensor)\n",
        "            s=s.type(torch.cuda.FloatTensor)   \n",
        "           # prediction on testing dataset\n",
        "            pred = model(u,p,s)\n",
        "            pred=pred.detach().cpu().numpy()\n",
        "            # evaluate the prediction accuracy\n",
        "            rmse = math.sqrt(mean_squared_error(y_test,pred))\n",
        "            print('Epoch :', epoch, '\\t', 'loss :', round(loss_train.item(), 3), '\\t',\n",
        "                  \"RMSE =\", rmse)\n",
        "            rmse_history.append(rmse)\n",
        "\n",
        "    return rmse_history"
      ],
      "metadata": {
        "id": "GEFYNph3x6yx"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_list=list()\n",
        "rmse_list=train(100,model,optimizer=optimizer,criterion=criterion)\n",
        "print(rmse_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FfZPVSwx-Bi",
        "outputId": "f17b3725-4371-4bef-e575-a5ded3da97e5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 10 \t loss : 6.857 \t RMSE = 3.716202576633326\n",
            "Epoch : 20 \t loss : 6.325 \t RMSE = 1.8402587681714186\n",
            "Epoch : 30 \t loss : 3.157 \t RMSE = 2.1963455104740226\n",
            "Epoch : 40 \t loss : 3.19 \t RMSE = 1.1257560097103847\n",
            "Epoch : 50 \t loss : 2.253 \t RMSE = 0.8929702598579038\n",
            "Epoch : 60 \t loss : 0.954 \t RMSE = 1.6247834281388065\n",
            "Epoch : 70 \t loss : 1.395 \t RMSE = 0.7388136220861687\n",
            "Epoch : 80 \t loss : 0.495 \t RMSE = 1.2499738213658862\n",
            "Epoch : 90 \t loss : 0.662 \t RMSE = 1.1342626826060933\n",
            "Epoch : 100 \t loss : 0.938 \t RMSE = 1.4311033648540579\n",
            "[3.716202576633326, 1.8402587681714186, 2.1963455104740226, 1.1257560097103847, 0.8929702598579038, 1.6247834281388065, 0.7388136220861687, 1.2499738213658862, 1.1342626826060933, 1.4311033648540579]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/캡스톤/FirstModel\")"
      ],
      "metadata": {
        "id": "MMVKzTX0yBPj"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시각화"
      ],
      "metadata": {
        "id": "lyZqTRBzmNpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "-BeiWbNflW6E"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4ZFppHtWmN5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}